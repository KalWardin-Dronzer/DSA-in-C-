{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN26uGa62iVnoMEOSK19AAM"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUkv8zM8oO1a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 22. Build a feedforward neural network  (NumPy only) for digit classification (MNIST).\n",
        "import os\n",
        "import gzip\n",
        "import struct\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "from time import time\n",
        "\n",
        "# ---------- HYPERPARAMS ----------\n",
        "HIDDEN_SIZES = [256, 128]\n",
        "LEARNING_RATE = 0.1\n",
        "MOMENTUM = 0.9  # set 0.0 to disable\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 128\n",
        "SEED = 42\n",
        "WEIGHT_SCALE = None  # if None, use Xavier/Glorot init\n",
        "VALIDATION_SPLIT = 0.1\n",
        "SAVE_PATH = 'mnist_model.npz'\n",
        "DOWNLOAD_DIR = 'mnist_data'\n",
        "\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# ---------- MNIST download + parsing (IDX format) ----------\n",
        "MNIST_FILES = {\n",
        "    'train_images': ('train-images-idx3-ubyte.gz',\n",
        "                     'https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz'),\n",
        "    'train_labels': ('train-labels-idx1-ubyte.gz',\n",
        "                     'https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz'),\n",
        "    'test_images': ('t10k-images-idx3-ubyte.gz',\n",
        "                    'https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz'),\n",
        "    'test_labels': ('t10k-labels-idx1-ubyte.gz',\n",
        "                    'https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz')\n",
        "}\n",
        "\n",
        "\n",
        "def ensure_mnist(download_dir=DOWNLOAD_DIR):\n",
        "    os.makedirs(download_dir, exist_ok=True)\n",
        "    for name, (fname, url) in MNIST_FILES.items():\n",
        "        path = os.path.join(download_dir, fname)\n",
        "        if not os.path.exists(path):\n",
        "            print(f'Downloading {fname}...')\n",
        "            urllib.request.urlretrieve(url, path)\n",
        "            print('Done')\n",
        "    print('All files present.')\n",
        "\n",
        "\n",
        "def parse_idx_images(gz_path):\n",
        "    with gzip.open(gz_path, 'rb') as f:\n",
        "        magic, num, rows, cols = struct.unpack('>IIII', f.read(16))\n",
        "        data = np.frombuffer(f.read(), dtype=np.uint8)\n",
        "        data = data.reshape(num, rows * cols).astype(np.float32) / 255.0\n",
        "        return data\n",
        "\n",
        "\n",
        "def parse_idx_labels(gz_path):\n",
        "    with gzip.open(gz_path, 'rb') as f:\n",
        "        magic, num = struct.unpack('>II', f.read(8))\n",
        "        data = np.frombuffer(f.read(), dtype=np.uint8)\n",
        "        return data.astype(np.int64)\n",
        "\n",
        "\n",
        "def load_mnist(download_dir=DOWNLOAD_DIR):\n",
        "    ensure_mnist(download_dir)\n",
        "    train_images = parse_idx_images(os.path.join(download_dir, MNIST_FILES['train_images'][0]))\n",
        "    train_labels = parse_idx_labels(os.path.join(download_dir, MNIST_FILES['train_labels'][0]))\n",
        "    test_images = parse_idx_images(os.path.join(download_dir, MNIST_FILES['test_images'][0]))\n",
        "    test_labels = parse_idx_labels(os.path.join(download_dir, MNIST_FILES['test_labels'][0]))\n",
        "    return train_images, train_labels, test_images, test_labels\n",
        "\n",
        "\n",
        "# ---------- Utilities ----------\n",
        "\n",
        "def one_hot(labels, num_classes=10):\n",
        "    y = np.zeros((labels.shape[0], num_classes), dtype=np.float32)\n",
        "    y[np.arange(labels.shape[0]), labels] = 1.0\n",
        "    return y\n",
        "\n",
        "\n",
        "def accuracy(pred_probs, labels):\n",
        "    preds = np.argmax(pred_probs, axis=1)\n",
        "    return np.mean(preds == labels)\n",
        "\n",
        "\n",
        "# Activation functions and derivatives\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def relu_grad(x):\n",
        "    return (x > 0).astype(x.dtype)\n",
        "\n",
        "\n",
        "def softmax_stable(x):\n",
        "    # x: (N, C)\n",
        "    x = x - np.max(x, axis=1, keepdims=True)\n",
        "    exp = np.exp(x)\n",
        "    return exp / np.sum(exp, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "# Loss (cross-entropy) and gradient w.r.t. logits (for softmax+CE)\n",
        "\n",
        "def cross_entropy_loss(probs, one_hot_targets):\n",
        "    # probs: (N, C), already softmaxed\n",
        "    N = probs.shape[0]\n",
        "    clipped = np.clip(probs, 1e-12, 1.0)\n",
        "    loss = -np.sum(one_hot_targets * np.log(clipped)) / N\n",
        "    return loss\n",
        "\n",
        "\n",
        "def grad_softmax_cross_entropy(probs, one_hot_targets):\n",
        "    # derivative of loss w.r.t. logits z when probs = softmax(z)\n",
        "    return (probs - one_hot_targets) / probs.shape[0]\n",
        "\n",
        "\n",
        "# ---------- Neural network class (NumPy only) ----------\n",
        "\n",
        "class FeedForwardNN:\n",
        "    def __init__(self, input_dim, hidden_sizes, output_dim, weight_scale=None):\n",
        "        sizes = [input_dim] + hidden_sizes + [output_dim]\n",
        "        self.num_layers = len(sizes) - 1\n",
        "        self.W = []\n",
        "        self.b = []\n",
        "        for i in range(self.num_layers):\n",
        "            fan_in, fan_out = sizes[i], sizes[i+1]\n",
        "            if weight_scale is None:\n",
        "                # Xavier/Glorot uniform\n",
        "                limit = np.sqrt(6.0 / (fan_in + fan_out))\n",
        "                W = np.random.uniform(-limit, limit, size=(fan_in, fan_out)).astype(np.float32)\n",
        "            else:\n",
        "                W = np.random.randn(fan_in, fan_out).astype(np.float32) * weight_scale\n",
        "            b = np.zeros((1, fan_out), dtype=np.float32)\n",
        "            self.W.append(W)\n",
        "            self.b.append(b)\n",
        "\n",
        "        # velocity for momentum\n",
        "        self.vW = [np.zeros_like(w) for w in self.W]\n",
        "        self.vb = [np.zeros_like(b) for b in self.b]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"Returns logits and caches activations for backprop.\"\"\"\n",
        "        activations = [X]\n",
        "        preacts = []\n",
        "        a = X\n",
        "        for i in range(self.num_layers - 1):  # hidden layers\n",
        "            z = a.dot(self.W[i]) + self.b[i]\n",
        "            preacts.append(z)\n",
        "            a = relu(z)\n",
        "            activations.append(a)\n",
        "        # final layer (logits)\n",
        "        z = a.dot(self.W[-1]) + self.b[-1]\n",
        "        preacts.append(z)\n",
        "        activations.append(z)  # store logits as last activation\n",
        "        return z, activations, preacts\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        logits, _, _ = self.forward(X)\n",
        "        return softmax_stable(logits)\n",
        "\n",
        "    def save(self, path=SAVE_PATH):\n",
        "        np.savez(path, *self.W, *self.b)\n",
        "        print(f'Saved model to {path}')\n",
        "\n",
        "    def load(self, path):\n",
        "        data = np.load(path)\n",
        "        arrs = [data[key] for key in data]\n",
        "        half = len(arrs) // 2\n",
        "        self.W = [arrs[i] for i in range(half)]\n",
        "        self.b = [arrs[i+half] for i in range(half)]\n",
        "        self.vW = [np.zeros_like(w) for w in self.W]\n",
        "        self.vb = [np.zeros_like(b) for b in self.b]\n",
        "\n",
        "    def update_params(self, grads_W, grads_b, lr, momentum):\n",
        "        for i in range(self.num_layers):\n",
        "            if momentum > 0:\n",
        "                self.vW[i] = momentum * self.vW[i] - lr * grads_W[i]\n",
        "                self.vb[i] = momentum * self.vb[i] - lr * grads_b[i]\n",
        "                self.W[i] += self.vW[i]\n",
        "                self.b[i] += self.vb[i]\n",
        "            else:\n",
        "                self.W[i] -= lr * grads_W[i]\n",
        "                self.b[i] -= lr * grads_b[i]\n",
        "\n",
        "    def backward(self, activations, preacts, logits, onehot_targets):\n",
        "        # compute gradients via backprop\n",
        "        grads_W = [None] * self.num_layers\n",
        "        grads_b = [None] * self.num_layers\n",
        "\n",
        "        # gradient at logits\n",
        "        probs = softmax_stable(logits)\n",
        "        delta = grad_softmax_cross_entropy(probs, onehot_targets)  # shape (N, C)\n",
        "\n",
        "        # last layer\n",
        "        a_prev = activations[-2]  # output of last hidden layer\n",
        "        grads_W[-1] = a_prev.T.dot(delta)\n",
        "        grads_b[-1] = np.sum(delta, axis=0, keepdims=True)\n",
        "\n",
        "        # backprop through hidden layers\n",
        "        delta_prev = delta\n",
        "        for l in range(self.num_layers - 2, -1, -1):\n",
        "            W_next = self.W[l+1]\n",
        "            # propagate\n",
        "            delta = delta_prev.dot(W_next.T) * relu_grad(preacts[l])\n",
        "            a_prev = activations[l]\n",
        "            grads_W[l] = a_prev.T.dot(delta)\n",
        "            grads_b[l] = np.sum(delta, axis=0, keepdims=True)\n",
        "            delta_prev = delta\n",
        "\n",
        "        return grads_W, grads_b\n",
        "\n",
        "\n",
        "# ---------- Training loop ----------\n",
        "\n",
        "def iterate_minibatches(X, y, batch_size, shuffle=True):\n",
        "    N = X.shape[0]\n",
        "    idx = np.arange(N)\n",
        "    if shuffle:\n",
        "        np.random.shuffle(idx)\n",
        "    for start in range(0, N, batch_size):\n",
        "        end = min(start + batch_size, N)\n",
        "        batch_idx = idx[start:end]\n",
        "        yield X[batch_idx], y[batch_idx]\n",
        "\n",
        "\n",
        "def train(model, X_train, y_train_onehot, y_train_labels, X_val, y_val_labels,\n",
        "          epochs, batch_size, lr, momentum):\n",
        "    N = X_train.shape[0]\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        t0 = time()\n",
        "        epoch_loss = 0.0\n",
        "        batches = 0\n",
        "        for X_batch, y_batch in iterate_minibatches(X_train, y_train_onehot, batch_size):\n",
        "            logits, activations, preacts = model.forward(X_batch)\n",
        "            probs = softmax_stable(logits)\n",
        "            loss = cross_entropy_loss(probs, y_batch)\n",
        "            grads_W, grads_b = model.backward(activations, preacts, logits, y_batch)\n",
        "            # normalize grads by batch size (already in grad function for softmax/CE), but grads_W are sums\n",
        "            # since our grad_softmax_cross_entropy normalized by N, grads_W currently are scaled by batch_size/N.\n",
        "            # To be safe, divide grads by 1.0 (they are already averaged because delta was averaged in grad func).\n",
        "            model.update_params(grads_W, grads_b, lr, momentum)\n",
        "\n",
        "            epoch_loss += loss\n",
        "            batches += 1\n",
        "\n",
        "        t1 = time()\n",
        "        train_probs = model.predict_proba(X_train)\n",
        "        train_acc = accuracy(train_probs, y_train_labels)\n",
        "        val_probs = model.predict_proba(X_val)\n",
        "        val_acc = accuracy(val_probs, y_val_labels)\n",
        "\n",
        "        print(f'Epoch {epoch}/{epochs} - loss: {epoch_loss / batches:.4f} - '\n",
        "              f'train acc: {train_acc*100:.2f}% - val acc: {val_acc*100:.2f}% - time: {t1-t0:.2f}s')\n",
        "\n",
        "\n",
        "# ---------- Main ----------\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print('Loading MNIST...')\n",
        "    X_train, y_train, X_test, y_test = load_mnist()\n",
        "\n",
        "    # shuffle train set and split validation\n",
        "    perm = np.random.permutation(X_train.shape[0])\n",
        "    X_train = X_train[perm]\n",
        "    y_train = y_train[perm]\n",
        "\n",
        "    val_size = int(len(X_train) * VALIDATION_SPLIT)\n",
        "    X_val = X_train[:val_size]\n",
        "    y_val = y_train[:val_size]\n",
        "    X_train = X_train[val_size:]\n",
        "    y_train = y_train[val_size:]\n",
        "\n",
        "    print('Shapes:')\n",
        "    print(' X_train', X_train.shape)\n",
        "    print(' y_train', y_train.shape)\n",
        "    print(' X_val', X_val.shape)\n",
        "    print(' X_test', X_test.shape)\n",
        "    print(' y_test', y_test.shape)\n",
        "\n",
        "    # Convert labels to one-hot\n",
        "    y_train_onehot = one_hot(y_train, 10)\n",
        "    y_val_onehot = one_hot(y_val, 10)\n",
        "    y_test_onehot = one_hot(y_test, 10)\n",
        "\n",
        "    model = FeedForwardNN(input_dim=784, hidden_sizes=HIDDEN_SIZES, output_dim=10, weight_scale=WEIGHT_SCALE)\n",
        "\n",
        "    print('Starting training...')\n",
        "    train(model,\n",
        "          X_train, y_train_onehot, y_train,\n",
        "          X_val, y_val,\n",
        "          epochs=EPOCHS,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          lr=LEARNING_RATE,\n",
        "          momentum=MOMENTUM)\n",
        "\n",
        "    # final test accuracy\n",
        "    test_probs = model.predict_proba(X_test)\n",
        "    test_acc = accuracy(test_probs, y_test)\n",
        "    print(f'Final test accuracy: {test_acc*100:.2f}%')\n",
        "\n",
        "    # save model\n",
        "    model.save(SAVE_PATH)\n",
        "\n",
        "    print('Done.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZaTAzOIyXQT",
        "outputId": "bf00dd1e-823d-4f5b-a8b1-59b269989544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading MNIST...\n",
            "Downloading train-images-idx3-ubyte.gz...\n",
            "Done\n",
            "Downloading train-labels-idx1-ubyte.gz...\n",
            "Done\n",
            "Downloading t10k-images-idx3-ubyte.gz...\n",
            "Done\n",
            "Downloading t10k-labels-idx1-ubyte.gz...\n",
            "Done\n",
            "All files present.\n",
            "Shapes:\n",
            " X_train (54000, 784)\n",
            " y_train (54000,)\n",
            " X_val (6000, 784)\n",
            " X_test (10000, 784)\n",
            " y_test (10000,)\n",
            "Starting training...\n",
            "Epoch 1/10 - loss: 0.2684 - train acc: 96.82% - val acc: 96.17% - time: 4.17s\n",
            "Epoch 2/10 - loss: 0.0969 - train acc: 97.42% - val acc: 96.45% - time: 1.65s\n",
            "Epoch 3/10 - loss: 0.0685 - train acc: 98.51% - val acc: 97.48% - time: 2.38s\n",
            "Epoch 4/10 - loss: 0.0502 - train acc: 98.97% - val acc: 97.65% - time: 1.86s\n",
            "Epoch 5/10 - loss: 0.0390 - train acc: 99.25% - val acc: 98.00% - time: 3.88s\n",
            "Epoch 6/10 - loss: 0.0278 - train acc: 98.60% - val acc: 97.33% - time: 1.76s\n",
            "Epoch 7/10 - loss: 0.0220 - train acc: 99.63% - val acc: 98.13% - time: 1.88s\n",
            "Epoch 8/10 - loss: 0.0187 - train acc: 99.58% - val acc: 98.12% - time: 1.78s\n",
            "Epoch 9/10 - loss: 0.0137 - train acc: 99.67% - val acc: 98.23% - time: 1.81s\n",
            "Epoch 10/10 - loss: 0.0109 - train acc: 99.72% - val acc: 97.90% - time: 4.37s\n",
            "Final test accuracy: 97.83%\n",
            "Saved model to mnist_model.npz\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 23. Implement a Convolutional Neural Network (CNN) using TensorFlow/Keras for CIFAR-10.\n",
        "\"\"\"\n",
        "CIFAR-10 CNN with TensorFlow / Keras\n",
        "\n",
        "Usage:\n",
        "    python cifar10_cnn_keras.py\n",
        "\n",
        "Features:\n",
        " - Loads CIFAR-10 from tf.keras.datasets\n",
        " - Builds a moderately deep CNN with Conv-BN-ReLU blocks\n",
        " - Uses data augmentation (random flip, translation, cutout optional)\n",
        " - Uses callbacks: ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        " - Saves the best model and prints test accuracy\n",
        "\n",
        "Requirements:\n",
        " - TensorFlow 2.x (tested on 2.12+)\n",
        "\n",
        "Tweak hyperparameters in the HYPERPARAMS block.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# ---------- HYPERPARAMS ----------\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 20\n",
        "NUM_CLASSES = 10\n",
        "INPUT_SHAPE = (32, 32, 3)\n",
        "LEARNING_RATE = 1e-3\n",
        "WEIGHT_DECAY = 1e-4\n",
        "MODEL_DIR = 'cifar10_cnn_model'\n",
        "SEED = 42\n",
        "AUGMENTATION = True\n",
        "\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# ---------- Data loading and preprocessing ----------\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "# Normalize to [0,1]\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# split off a validation set from train\n",
        "val_fraction = 0.1\n",
        "num_val = int(len(x_train) * val_fraction)\n",
        "indices = np.arange(len(x_train))\n",
        "np.random.shuffle(indices)\n",
        "val_idx = indices[:num_val]\n",
        "train_idx = indices[num_val:]\n",
        "\n",
        "x_val = x_train[val_idx]\n",
        "y_val = y_train[val_idx]\n",
        "\n",
        "x_train = x_train[train_idx]\n",
        "y_train = y_train[train_idx]\n",
        "\n",
        "print('Train / Val / Test shapes:', x_train.shape, x_val.shape, x_test.shape)\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train_cat = keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
        "y_val_cat = keras.utils.to_categorical(y_val, NUM_CLASSES)\n",
        "y_test_cat = keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
        "\n",
        "# ---------- Data augmentation pipeline (tf.keras.layers) ----------\n",
        "if AUGMENTATION:\n",
        "    data_augmentation = keras.Sequential([\n",
        "        layers.RandomFlip('horizontal'),\n",
        "        layers.RandomTranslation(0.1, 0.1),\n",
        "        layers.RandomRotation(0.05),\n",
        "    ], name='data_augmentation')\n",
        "else:\n",
        "    data_augmentation = keras.Sequential([], name='data_augmentation')\n",
        "\n",
        "# Optional Cutout function\n",
        "@tf.function\n",
        "def random_cutout(images, mask_size=8):\n",
        "    # images: [B, H, W, C]\n",
        "    batch_size = tf.shape(images)[0]\n",
        "    img_h = tf.shape(images)[1]\n",
        "    img_w = tf.shape(images)[2]\n",
        "\n",
        "    y = tf.random.uniform([batch_size], 0, img_h, dtype=tf.int32)\n",
        "    x = tf.random.uniform([batch_size], 0, img_w, dtype=tf.int32)\n",
        "\n",
        "    y1 = tf.clip_by_value(y - mask_size // 2, 0, img_h)\n",
        "    x1 = tf.clip_by_value(x - mask_size // 2, 0, img_w)\n",
        "    y2 = tf.clip_by_value(y1 + mask_size, 0, img_h)\n",
        "    x2 = tf.clip_by_value(x1 + mask_size, 0, img_w)\n",
        "\n",
        "    masks = tf.ones([batch_size, img_h, img_w, 1], dtype=images.dtype)\n",
        "    for i in range(batch_size):\n",
        "        masks = tf.tensor_scatter_nd_update(masks,\n",
        "                                            indices=[[i, y1[i], x1[i], 0]],\n",
        "                                            updates=[0.0])\n",
        "    # This is a simplified cutout placeholder (not perfect). For production use, vectorize properly.\n",
        "    return images * masks\n",
        "\n",
        "# ---------- Model definition ----------\n",
        "\n",
        "def conv_block(x, filters, kernel_size=3, pool=True):\n",
        "    x = layers.Conv2D(filters, kernel_size, padding='same', use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.Conv2D(filters, kernel_size, padding='same', use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    if pool:\n",
        "        x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def build_model(input_shape=INPUT_SHAPE, num_classes=NUM_CLASSES, weight_decay=WEIGHT_DECAY):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "\n",
        "    x = data_augmentation(inputs)\n",
        "\n",
        "    # stem\n",
        "    x = layers.Conv2D(64, 3, padding='same', use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    # conv blocks\n",
        "    x = conv_block(x, 64)\n",
        "    x = conv_block(x, 128)\n",
        "    x = conv_block(x, 256, pool=True)\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(256, use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name='cifar10_cnn')\n",
        "    return model\n",
        "\n",
        "# Build model\n",
        "model = build_model()\n",
        "model.summary()\n",
        "\n",
        "# ---------- Optimizer, loss, metrics ----------\n",
        "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "loss = keras.losses.CategoricalCrossentropy()\n",
        "metrics = ['accuracy']\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "# ---------- Callbacks ----------\n",
        "now = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "checkpoint_path = os.path.join(MODEL_DIR, f'best_model_{now}.h5')\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(checkpoint_path, monitor='val_accuracy', save_best_only=True, verbose=1),\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1),\n",
        "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=12, verbose=1, restore_best_weights=True)\n",
        "]\n",
        "\n",
        "# ---------- Training ----------\n",
        "# Use tf.data for performance\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train_cat))\n",
        "train_ds = train_ds.shuffle(10000, seed=SEED)\n",
        "train_ds = train_ds.batch(BATCH_SIZE)\n",
        "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val_cat))\n",
        "val_ds = val_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# If you want to apply augmentation per-batch using the Keras layer, keep as-is. Alternatively, map augmentation:\n",
        "if AUGMENTATION:\n",
        "    # map augmentation on the dataset (applies the Sequential augmentation layers)\n",
        "    train_ds = train_ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_ds,\n",
        "    callbacks=callbacks,\n",
        ")\n",
        "\n",
        "# ---------- Evaluation ----------\n",
        "print('\\nEvaluating on test set...')\n",
        "# load best model if checkpoint saved\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print('Loading best model from checkpoint...')\n",
        "    model = keras.models.load_model(checkpoint_path)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test_cat, batch_size=BATCH_SIZE)\n",
        "print(f'Test loss: {test_loss:.4f} - Test accuracy: {test_acc*100:.2f}%')\n",
        "\n",
        "# Save final model\n",
        "final_path = os.path.join(MODEL_DIR, f'final_model_{now}.h5')\n",
        "model.save(final_path)\n",
        "print(f'Final model saved to {final_path}')\n",
        "\n",
        "print('Done.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TUMFj4mr7mZB",
        "outputId": "e41c4076-365f-4975-f6b5-b5c560bc8cb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 1us/step\n",
            "Train / Val / Test shapes: (45000, 32, 32, 3) (5000, 32, 32, 3) (10000, 32, 32, 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"cifar10_cnn\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"cifar10_cnn\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ data_augmentation (\u001b[38;5;33mSequential\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │         \u001b[38;5;34m1,728\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m36,864\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m36,864\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_2 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,728\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_3 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m147,456\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_4 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m294,912\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │         \u001b[38;5;34m1,024\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_5 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m589,824\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │         \u001b[38;5;34m1,024\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_6 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m65,536\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_7 (\u001b[38;5;33mActivation\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m2,570\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ data_augmentation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,728</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,864</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,728</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,456</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">294,912</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,536</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ activation_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,254,346\u001b[0m (4.78 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,254,346</span> (4.78 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,251,914\u001b[0m (4.78 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,251,914</span> (4.78 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,432\u001b[0m (9.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,432</span> (9.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.3152 - loss: 1.9866\n",
            "Epoch 1: val_accuracy improved from -inf to 0.14280, saving model to cifar10_cnn_model/best_model_20251117-135615.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 132ms/step - accuracy: 0.3156 - loss: 1.9852 - val_accuracy: 0.1428 - val_loss: 3.1388 - learning_rate: 0.0010\n",
            "Epoch 2/20\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.5042 - loss: 1.3870\n",
            "Epoch 2: val_accuracy improved from 0.14280 to 0.29240, saving model to cifar10_cnn_model/best_model_20251117-135615.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 123ms/step - accuracy: 0.5043 - loss: 1.3866 - val_accuracy: 0.2924 - val_loss: 2.3289 - learning_rate: 0.0010\n",
            "Epoch 3/20\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.5830 - loss: 1.1699\n",
            "Epoch 3: val_accuracy improved from 0.29240 to 0.37460, saving model to cifar10_cnn_model/best_model_20251117-135615.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 123ms/step - accuracy: 0.5831 - loss: 1.1697 - val_accuracy: 0.3746 - val_loss: 2.4442 - learning_rate: 0.0010\n",
            "Epoch 4/20\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.6451 - loss: 1.0152\n",
            "Epoch 4: val_accuracy improved from 0.37460 to 0.49460, saving model to cifar10_cnn_model/best_model_20251117-135615.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 124ms/step - accuracy: 0.6452 - loss: 1.0151 - val_accuracy: 0.4946 - val_loss: 1.7309 - learning_rate: 0.0010\n",
            "Epoch 5/20\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.6745 - loss: 0.9272\n",
            "Epoch 5: val_accuracy improved from 0.49460 to 0.60220, saving model to cifar10_cnn_model/best_model_20251117-135615.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 124ms/step - accuracy: 0.6746 - loss: 0.9271 - val_accuracy: 0.6022 - val_loss: 1.1504 - learning_rate: 0.0010\n",
            "Epoch 6/20\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.7114 - loss: 0.8350\n",
            "Epoch 6: val_accuracy did not improve from 0.60220\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 124ms/step - accuracy: 0.7114 - loss: 0.8350 - val_accuracy: 0.5452 - val_loss: 1.5890 - learning_rate: 0.0010\n",
            "Epoch 7/20\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.7275 - loss: 0.7866\n",
            "Epoch 7: val_accuracy did not improve from 0.60220\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 124ms/step - accuracy: 0.7275 - loss: 0.7866 - val_accuracy: 0.5544 - val_loss: 1.7819 - learning_rate: 0.0010\n",
            "Epoch 8/20\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.7384 - loss: 0.7522\n",
            "Epoch 8: val_accuracy did not improve from 0.60220\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 125ms/step - accuracy: 0.7385 - loss: 0.7521 - val_accuracy: 0.5898 - val_loss: 1.2166 - learning_rate: 0.0010\n",
            "Epoch 9/20\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.7624 - loss: 0.6926\n",
            "Epoch 9: val_accuracy improved from 0.60220 to 0.62920, saving model to cifar10_cnn_model/best_model_20251117-135615.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 126ms/step - accuracy: 0.7624 - loss: 0.6926 - val_accuracy: 0.6292 - val_loss: 1.2730 - learning_rate: 0.0010\n",
            "Epoch 10/20\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.7667 - loss: 0.6770\n",
            "Epoch 10: val_accuracy improved from 0.62920 to 0.74540, saving model to cifar10_cnn_model/best_model_20251117-135615.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 126ms/step - accuracy: 0.7667 - loss: 0.6769 - val_accuracy: 0.7454 - val_loss: 0.7500 - learning_rate: 0.0010\n",
            "Epoch 11/20\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.7841 - loss: 0.6333\n",
            "Epoch 11: val_accuracy did not improve from 0.74540\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 126ms/step - accuracy: 0.7841 - loss: 0.6333 - val_accuracy: 0.6510 - val_loss: 1.0869 - learning_rate: 0.0010\n",
            "Epoch 12/20\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.7887 - loss: 0.6061\n",
            "Epoch 12: val_accuracy did not improve from 0.74540\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 126ms/step - accuracy: 0.7887 - loss: 0.6060 - val_accuracy: 0.7274 - val_loss: 0.7609 - learning_rate: 0.0010\n",
            "Epoch 13/20\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8024 - loss: 0.5856\n",
            "Epoch 13: val_accuracy did not improve from 0.74540\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 126ms/step - accuracy: 0.8024 - loss: 0.5856 - val_accuracy: 0.7120 - val_loss: 0.8266 - learning_rate: 0.0010\n",
            "Epoch 14/20\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.8046 - loss: 0.5596\n",
            "Epoch 14: val_accuracy did not improve from 0.74540\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 127ms/step - accuracy: 0.8046 - loss: 0.5596 - val_accuracy: 0.7248 - val_loss: 0.8422 - learning_rate: 0.0010\n",
            "Epoch 15/20\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8164 - loss: 0.5405\n",
            "Epoch 15: val_accuracy improved from 0.74540 to 0.74980, saving model to cifar10_cnn_model/best_model_20251117-135615.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 127ms/step - accuracy: 0.8164 - loss: 0.5405 - val_accuracy: 0.7498 - val_loss: 0.7480 - learning_rate: 0.0010\n",
            "Epoch 16/20\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8212 - loss: 0.5201\n",
            "Epoch 16: val_accuracy did not improve from 0.74980\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 126ms/step - accuracy: 0.8212 - loss: 0.5202 - val_accuracy: 0.6910 - val_loss: 0.9481 - learning_rate: 0.0010\n",
            "Epoch 17/20\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8245 - loss: 0.5005\n",
            "Epoch 17: val_accuracy did not improve from 0.74980\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 127ms/step - accuracy: 0.8245 - loss: 0.5005 - val_accuracy: 0.7456 - val_loss: 0.7360 - learning_rate: 0.0010\n",
            "Epoch 18/20\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8333 - loss: 0.4906\n",
            "Epoch 18: val_accuracy did not improve from 0.74980\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 126ms/step - accuracy: 0.8333 - loss: 0.4906 - val_accuracy: 0.6384 - val_loss: 1.3504 - learning_rate: 0.0010\n",
            "Epoch 19/20\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8364 - loss: 0.4736\n",
            "Epoch 19: val_accuracy improved from 0.74980 to 0.75500, saving model to cifar10_cnn_model/best_model_20251117-135615.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 127ms/step - accuracy: 0.8364 - loss: 0.4736 - val_accuracy: 0.7550 - val_loss: 0.7332 - learning_rate: 0.0010\n",
            "Epoch 20/20\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.8426 - loss: 0.4525\n",
            "Epoch 20: val_accuracy did not improve from 0.75500\n",
            "\u001b[1m176/176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 126ms/step - accuracy: 0.8426 - loss: 0.4526 - val_accuracy: 0.6826 - val_loss: 1.0697 - learning_rate: 0.0010\n",
            "Restoring model weights from the end of the best epoch: 19.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluating on test set...\n",
            "Loading best model from checkpoint...\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 34ms/step - accuracy: 0.7569 - loss: 0.7572\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.7499 - Test accuracy: 75.93%\n",
            "Final model saved to cifar10_cnn_model/final_model_20251117-135615.h5\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2292a684"
      },
      "source": [
        "### Why is CNN training taking so long?\n",
        "\n",
        "Training deep Convolutional Neural Networks (CNNs) like the one in the previous cell is computationally intensive. Here are the primary reasons and how to address them:\n",
        "\n",
        "1.  **Computational Power**: Deep learning models require significant processing power. If you are running on a CPU runtime, it will be much slower than on a GPU.\n",
        "    *   **Solution**: Ensure you are using a **GPU runtime** in Colab. Go to `Runtime` > `Change runtime type` and select `GPU` as the hardware accelerator.\n",
        "\n",
        "2.  **Number of Epochs**: The current configuration trains for `80` epochs. Each epoch involves processing the entire dataset multiple times for both forward and backward passes.\n",
        "    *   **Solution**: For quicker experimentation, you can reduce the `EPOCHS` hyperparameter (e.g., to `5` or `10`) in the `HYPERPARAMS` section of the code.\n",
        "\n",
        "3.  **Data Augmentation**: Data augmentation techniques (like `RandomFlip`, `RandomTranslation`, `RandomRotation`) are applied to each image during training. While beneficial for model generalization, they add processing time per batch.\n",
        "\n",
        "By switching to a GPU runtime, you should see a drastic improvement in training speed. Reducing the number of epochs is also a good temporary measure for faster feedback."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 24. Train an RNN (LSTM) for next-word prediction on a text dataset.\n",
        "\"\"\"\n",
        "Next-word prediction with LSTM (TensorFlow / Keras)\n",
        "\n",
        "- Downloads a small text dataset (Shakespeare) and trains a word-level LSTM that\n",
        "  predicts the next word given a short context (seq_len words -> next word).\n",
        "- Saves the trained model and the tokenizer for later inference.\n",
        "- Includes a `generate_text` helper that samples next words with temperature.\n",
        "\n",
        "Run:\n",
        "    python nextword_lstm_keras.py\n",
        "\n",
        "Requirements:\n",
        "    tensorflow (2.x), numpy\n",
        "\n",
        "You can change DATA_URL to point to your own .txt file.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# ----------------- HYPERPARAMS -----------------\n",
        "DATA_URL = 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt'\n",
        "SEQ_LEN = 6          # number of input words (context length)\n",
        "BATCH_SIZE = 128\n",
        "EMBED_DIM = 128\n",
        "LSTM_UNITS = 256\n",
        "EPOCHS = 15\n",
        "BUFFER_SIZE = 10000\n",
        "LEARNING_RATE = 1e-3\n",
        "VALIDATION_SPLIT = 0.1\n",
        "MODEL_DIR = 'lstm_nextword_model'\n",
        "TOKENIZER_PATH = os.path.join(MODEL_DIR, 'tokenizer.pkl')\n",
        "SEED = 42\n",
        "\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "# ----------------- Download + read dataset -----------------\n",
        "print('Downloading dataset...')\n",
        "path = keras.utils.get_file('shakespeare.txt', DATA_URL)\n",
        "with open(path, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "print(f'Dataset length (chars): {len(text)}')\n",
        "\n",
        "# ----------------- Preprocessing (word-level) -----------------\n",
        "# Basic cleanup: collapse whitespace, keep punctuation as separate tokens\n",
        "text = text.lower()\n",
        "text = re.sub(r\"\\s+\", ' ', text)\n",
        "\n",
        "# Use Keras Tokenizer for word-level tokenization\n",
        "print('Tokenizing (word-level)...')\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(filters='')  # keep punctuation as tokens if present\n",
        "# fit on texts split by whitespace to ensure word-level tokens\n",
        "tokens = text.split(' ')\n",
        "# join with single spaces to keep consistent formatting for tokenizer\n",
        "tokenizer.fit_on_texts([' '.join(tokens)])\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocab size:', vocab_size)\n",
        "\n",
        "# Convert full text to sequence of integer tokens\n",
        "seq = tokenizer.texts_to_sequences([' '.join(tokens)])[0]\n",
        "seq = np.array(seq, dtype=np.int32)\n",
        "\n",
        "# Build dataset of (seq_len -> next_word) pairs using sliding window\n",
        "inputs = []\n",
        "targets = []\n",
        "for i in range(len(seq) - SEQ_LEN):\n",
        "    inputs.append(seq[i:i+SEQ_LEN])\n",
        "    targets.append(seq[i+SEQ_LEN])\n",
        "inputs = np.array(inputs, dtype=np.int32)\n",
        "targets = np.array(targets, dtype=np.int32)\n",
        "print('Total examples:', inputs.shape[0])\n",
        "\n",
        "# Shuffle and split\n",
        "indices = np.arange(inputs.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "inputs = inputs[indices]\n",
        "targets = targets[indices]\n",
        "\n",
        "val_size = int(inputs.shape[0] * VALIDATION_SPLIT)\n",
        "X_val = inputs[:val_size]\n",
        "y_val = targets[:val_size]\n",
        "X_train = inputs[val_size:]\n",
        "y_train = targets[val_size:]\n",
        "print('Train / Val shapes:', X_train.shape, X_val.shape)\n",
        "\n",
        "# ----------------- tf.data pipelines -----------------\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "train_ds = train_ds.shuffle(BUFFER_SIZE, seed=SEED).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# ----------------- Model -----------------\n",
        "print('Building model...')\n",
        "inputs_layer = keras.Input(shape=(SEQ_LEN,), dtype='int32')\n",
        "# embedding maps token ids -> vectors\n",
        "x = layers.Embedding(input_dim=vocab_size, output_dim=EMBED_DIM, input_length=SEQ_LEN)(inputs_layer)\n",
        "# you can stack LSTMs or use Bidirectional if desired\n",
        "x = layers.LSTM(LSTM_UNITS, return_sequences=False)(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "outputs = layers.Dense(vocab_size, activation='softmax')(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs_layer, outputs=outputs)\n",
        "model.summary()\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "loss = keras.losses.SparseCategoricalCrossentropy()\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "# ----------------- Callbacks -----------------\n",
        "checkpoint_path = os.path.join(MODEL_DIR, 'best_lstm_nextword.h5')\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(checkpoint_path, save_best_only=True, monitor='val_loss', verbose=1),\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n",
        "]\n",
        "\n",
        "# ----------------- Train -----------------\n",
        "print('Starting training...')\n",
        "history = model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, callbacks=callbacks)\n",
        "\n",
        "# Load best model\n",
        "if os.path.exists(checkpoint_path):\n",
        "    model = keras.models.load_model(checkpoint_path)\n",
        "\n",
        "# Save tokenizer\n",
        "with open(TOKENIZER_PATH, 'wb') as f:\n",
        "    pickle.dump(tokenizer, f)\n",
        "print('Saved tokenizer to', TOKENIZER_PATH)\n",
        "\n",
        "# Save final model\n",
        "final_model_path = os.path.join(MODEL_DIR, 'final_model.h5')\n",
        "model.save(final_model_path)\n",
        "print('Saved final model to', final_model_path)\n",
        "\n",
        "# ----------------- Inference helper: generate text -----------------\n",
        "\n",
        "def sample_from_probs(probs, temperature=1.0):\n",
        "    # probs: 1D numpy array over vocab\n",
        "    probs = np.asarray(probs).astype('float64')\n",
        "    if temperature <= 0:\n",
        "        return np.argmax(probs)\n",
        "    probs = np.log(probs + 1e-12) / temperature\n",
        "    exp = np.exp(probs - np.max(probs))\n",
        "    probs = exp / np.sum(exp)\n",
        "    return np.random.choice(len(probs), p=probs)\n",
        "\n",
        "\n",
        "def generate_text(seed_text, num_words=20, temperature=1.0):\n",
        "    # seed_text: string of words (can be shorter than SEQ_LEN)\n",
        "    with open(TOKENIZER_PATH, 'rb') as f:\n",
        "        tk = pickle.load(f)\n",
        "    words = seed_text.lower().split()\n",
        "    for _ in range(num_words):\n",
        "        # build input sequence of last SEQ_LEN tokens\n",
        "        seq_tokens = tk.texts_to_sequences([' '.join(words)])[0]\n",
        "        if len(seq_tokens) < SEQ_LEN:\n",
        "            pad = [0] * (SEQ_LEN - len(seq_tokens))\n",
        "            input_seq = np.array([pad + seq_tokens])\n",
        "        else:\n",
        "            input_seq = np.array([seq_tokens[-SEQ_LEN:]])\n",
        "        preds = model.predict(input_seq, verbose=0)[0]\n",
        "        next_id = sample_from_probs(preds, temperature)\n",
        "        # map id back to word (tokenizer.word_index is word->id)\n",
        "        # build inverse mapping\n",
        "        inv_map = {v: k for k, v in tk.word_index.items()}\n",
        "        next_word = inv_map.get(next_id, '')\n",
        "        if next_word == '':\n",
        "            break\n",
        "        words.append(next_word)\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Example generation\n",
        "print('\\nExample generations:')\n",
        "seed = 'to be or not to be'\n",
        "print('Seed:', seed)\n",
        "print('Generated (temp=0.8):', generate_text(seed, num_words=30, temperature=0.8))\n",
        "print('Generated (temp=1.2):', generate_text(seed, num_words=30, temperature=1.2))\n",
        "\n",
        "print('\\nDone.')\n"
      ],
      "metadata": {
        "id": "yuMrASZ-7mWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 25. Write code for Q-learning in a simple grid-world environment.\n",
        "\"\"\"\n",
        "Simple Grid-World + Q-Learning (pure Python + NumPy)\n",
        "\n",
        "Run this file to train a Q-learning agent to navigate a small grid world.\n",
        "Features:\n",
        " - customizable grid size, start, goal, and obstacles\n",
        " - epsilon-greedy policy, learning rate, discount factor\n",
        " - tracks episodic returns and success rate\n",
        " - prints learned policy and Q-table\n",
        " - optional visual rendering in terminal\n",
        "\n",
        "Usage:\n",
        "    python q_learning_gridworld.py\n",
        "\n",
        "This is intentionally dependency-light (only NumPy and matplotlib optional for plotting).\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ----------------------- Environment -----------------------\n",
        "class GridWorld:\n",
        "    \"\"\"A simple deterministic grid world.\n",
        "    States are (row, col). Actions: 0=up,1=right,2=down,3=left.\n",
        "    Rewards: step_reward for each step, goal_reward at reaching goal, and obstacle penalty.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_rows=5, n_cols=5, start=(0,0), goal=(4,4), obstacles=None,\n",
        "                 step_reward=-0.1, goal_reward=1.0, obstacle_reward=-1.0, max_steps=100):\n",
        "        self.n_rows = n_rows\n",
        "        self.n_cols = n_cols\n",
        "        self.start = start\n",
        "        self.state = start\n",
        "        self.goal = goal\n",
        "        self.obstacles = set(obstacles) if obstacles is not None else set()\n",
        "        self.step_reward = step_reward\n",
        "        self.goal_reward = goal_reward\n",
        "        self.obstacle_reward = obstacle_reward\n",
        "        self.max_steps = max_steps\n",
        "        self.steps = 0\n",
        "\n",
        "        # action space and state space sizes\n",
        "        self.n_actions = 4\n",
        "        self.n_states = n_rows * n_cols\n",
        "\n",
        "    def state_to_index(self, state):\n",
        "        r, c = state\n",
        "        return r * self.n_cols + c\n",
        "\n",
        "    def index_to_state(self, idx):\n",
        "        r = idx // self.n_cols\n",
        "        c = idx % self.n_cols\n",
        "        return (r, c)\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.start\n",
        "        self.steps = 0\n",
        "        return self.state_to_index(self.state)\n",
        "\n",
        "    def in_bounds(self, r, c):\n",
        "        return 0 <= r < self.n_rows and 0 <= c < self.n_cols\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Take action and return: next_state_index, reward, done, info\"\"\"\n",
        "        r, c = self.state\n",
        "        if action == 0:  # up\n",
        "            nr, nc = r - 1, c\n",
        "        elif action == 1:  # right\n",
        "            nr, nc = r, c + 1\n",
        "        elif action == 2:  # down\n",
        "            nr, nc = r + 1, c\n",
        "        elif action == 3:  # left\n",
        "            nr, nc = r, c - 1\n",
        "        else:\n",
        "            raise ValueError('Invalid action')\n",
        "\n",
        "        # if out of bounds, stay in place\n",
        "        if not self.in_bounds(nr, nc):\n",
        "            nr, nc = r, c\n",
        "\n",
        "        self.state = (nr, nc)\n",
        "        self.steps += 1\n",
        "\n",
        "        # compute reward\n",
        "        if self.state == self.goal:\n",
        "            reward = self.goal_reward\n",
        "            done = True\n",
        "        elif self.state in self.obstacles:\n",
        "            reward = self.obstacle_reward\n",
        "            done = False\n",
        "        else:\n",
        "            reward = self.step_reward\n",
        "            done = False\n",
        "\n",
        "        # episode ends if too many steps\n",
        "        if self.steps >= self.max_steps:\n",
        "            done = True\n",
        "\n",
        "        return self.state_to_index(self.state), reward, done, {}\n",
        "\n",
        "    def render(self, policy=None):\n",
        "        \"\"\"Print grid, agent position (A), goal (G), obstacles (X). Optionally show policy arrows.\"\"\"\n",
        "        grid = [['.' for _ in range(self.n_cols)] for _ in range(self.n_rows)]\n",
        "        for (orow, ocol) in self.obstacles:\n",
        "            grid[orow][ocol] = 'X'\n",
        "        gr, gc = self.goal\n",
        "        grid[gr][gc] = 'G'\n",
        "        ar, ac = self.state\n",
        "        grid[ar][ac] = 'A'\n",
        "\n",
        "        if policy is not None:\n",
        "            # policy is array of action ints for each state index\n",
        "            arrows = {0: '^', 1: '>', 2: 'v', 3: '<'}\n",
        "            print('Policy map (arrows show greedy action):')\n",
        "            for r in range(self.n_rows):\n",
        "                rowstr = ''\n",
        "                for c in range(self.n_cols):\n",
        "                    sidx = self.state_to_index((r,c))\n",
        "                    if (r,c) == self.goal:\n",
        "                        rowstr += ' G '\n",
        "                    elif (r,c) in self.obstacles:\n",
        "                        rowstr += ' X '\n",
        "                    else:\n",
        "                        rowstr += f' {arrows.get(policy[sidx], \".\")} '\n",
        "                print(rowstr)\n",
        "            print()\n",
        "\n",
        "        print('Grid:')\n",
        "        for r in range(self.n_rows):\n",
        "            print(' '.join(grid[r]))\n",
        "        print()\n",
        "\n",
        "# ----------------------- Q-Learning Agent -----------------------\n",
        "class QLearningAgent:\n",
        "    def __init__(self, n_states, n_actions, lr=0.1, gamma=0.99, epsilon=1.0, epsilon_min=0.05, epsilon_decay=0.995):\n",
        "        self.n_states = n_states\n",
        "        self.n_actions = n_actions\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "\n",
        "        # initialize Q-table to zeros\n",
        "        self.Q = np.zeros((n_states, n_actions), dtype=np.float32)\n",
        "\n",
        "    def get_action(self, state_idx):\n",
        "        # epsilon-greedy\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randrange(self.n_actions)\n",
        "        else:\n",
        "            return int(np.argmax(self.Q[state_idx]))\n",
        "\n",
        "    def update(self, s_idx, a, r, s_next_idx, done):\n",
        "        q = self.Q[s_idx, a]\n",
        "        if done:\n",
        "            target = r\n",
        "        else:\n",
        "            target = r + self.gamma * np.max(self.Q[s_next_idx])\n",
        "        self.Q[s_idx, a] = q + self.lr * (target - q)\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "            if self.epsilon < self.epsilon_min:\n",
        "                self.epsilon = self.epsilon_min\n",
        "\n",
        "    def greedy_policy(self):\n",
        "        return np.argmax(self.Q, axis=1)\n",
        "\n",
        "# ----------------------- Training loop -----------------------\n",
        "\n",
        "def train_q_learning(env, agent, episodes=500, max_steps_per_episode=100, render_every=0):\n",
        "    rewards = []\n",
        "    successes = []\n",
        "\n",
        "    for ep in range(1, episodes+1):\n",
        "        s = env.reset()\n",
        "        total_reward = 0.0\n",
        "        done = False\n",
        "        for t in range(max_steps_per_episode):\n",
        "            a = agent.get_action(s)\n",
        "            s_next, r, done, _ = env.step(a)\n",
        "            agent.update(s, a, r, s_next, done)\n",
        "            s = s_next\n",
        "            total_reward += r\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        agent.decay_epsilon()\n",
        "        rewards.append(total_reward)\n",
        "        successes.append(1.0 if env.state == env.goal else 0.0)\n",
        "\n",
        "        if render_every > 0 and ep % render_every == 0:\n",
        "            print(f'Episode {ep} - total_reward: {total_reward:.2f} - epsilon: {agent.epsilon:.3f} - success: {successes[-1]}')\n",
        "\n",
        "    return rewards, successes\n",
        "\n",
        "# ----------------------- Example usage -----------------------\n",
        "if __name__ == '__main__':\n",
        "    # build a simple 6x6 grid with obstacles\n",
        "    n_rows, n_cols = 6, 6\n",
        "    start = (0, 0)\n",
        "    goal = (5, 5)\n",
        "    obstacles = {(1,1), (2,1), (3,1), (4,1), (4,2), (4,3)}  # a wall with a gap\n",
        "\n",
        "    env = GridWorld(n_rows=n_rows, n_cols=n_cols, start=start, goal=goal, obstacles=obstacles,\n",
        "                    step_reward=-0.04, goal_reward=1.0, obstacle_reward=-1.0, max_steps=200)\n",
        "\n",
        "    agent = QLearningAgent(n_states=env.n_states, n_actions=env.n_actions,\n",
        "                           lr=0.5, gamma=0.98, epsilon=1.0, epsilon_min=0.05, epsilon_decay=0.995)\n",
        "\n",
        "    episodes = 1500\n",
        "    rewards, successes = train_q_learning(env, agent, episodes=episodes, max_steps_per_episode=200, render_every=100)\n",
        "\n",
        "    # plot learning curve\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(rewards)\n",
        "    plt.title('Episode reward')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Total reward')\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    # moving average success rate\n",
        "    window = 50\n",
        "    success_ma = np.convolve(successes, np.ones(window)/window, mode='valid')\n",
        "    plt.plot(success_ma)\n",
        "    plt.title('Success rate (moving avg)')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Success rate')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Show learned policy\n",
        "    policy = agent.greedy_policy()\n",
        "    print('\\nLearned greedy policy (arrows: ^ > v <):')\n",
        "    env.render(policy=policy)\n",
        "\n",
        "    # Print Q-values for start state\n",
        "    start_idx = env.state_to_index(start)\n",
        "    print('Q-values at start state:', agent.Q[start_idx])\n",
        "\n",
        "    # Demonstrate an episode following greedy policy\n",
        "    s = env.reset()\n",
        "    env.render()\n",
        "    print('Greedy rollout:')\n",
        "    for t in range(30):\n",
        "        a = int(policy[s])\n",
        "        s_next, r, done, _ = env.step(a)\n",
        "        env.render()\n",
        "        if done:\n",
        "            print('Episode finished, reached goal' if env.state == env.goal else 'Episode finished (max steps)')\n",
        "            break\n",
        "        s = s_next\n",
        "\n",
        "    print('Done.')\n"
      ],
      "metadata": {
        "id": "xEoEL0JO7mTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#26. Implement backpropagation and gradient descent for a small neural network (NumPy only).\n",
        "\"\"\"\n",
        "Backpropagation + Gradient Descent (NumPy-only)\n",
        "\n",
        "This script implements a small fully-connected neural network from scratch using only NumPy.\n",
        "It performs manual forward and backward passes (no autograd) and trains with mini-batch\n",
        "gradient descent and optional momentum.\n",
        "\n",
        "Features:\n",
        " - Dense layer implementation with Xavier init\n",
        " - Activation functions: sigmoid, tanh, ReLU, softmax\n",
        " - Losses: Mean Squared Error (MSE) and Cross-Entropy (with softmax)\n",
        " - Full vectorized forward/backward passes\n",
        " - Gradient checking utility (finite differences) to verify backprop\n",
        " - Small examples: learn XOR (binary) and a synthetic 3-class classification problem\n",
        "\n",
        "Usage:\n",
        "    python backprop_numpy.py\n",
        "\n",
        "Tweak hyperparameters in the HYPERPARAMS section.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# ------------------- HYPERPARAMS -------------------\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "HIDDEN_SIZES = [8]          # list of hidden layer sizes (e.g. [8, 8])\n",
        "ACTIVATION = 'tanh'         # 'sigmoid', 'tanh', 'relu'\n",
        "LOSS = 'cross_entropy'      # 'mse' or 'cross_entropy'\n",
        "LEARNING_RATE = 0.1\n",
        "MOMENTUM = 0.9\n",
        "EPOCHS = 1000\n",
        "BATCH_SIZE = 4\n",
        "PRINT_EVERY = 100\n",
        "GRAD_CHECK = False          # set True to run gradient check on small batch\n",
        "\n",
        "# ------------------- Utility functions -------------------\n",
        "\n",
        "def one_hot(y, num_classes):\n",
        "    Y = np.zeros((len(y), num_classes))\n",
        "    Y[np.arange(len(y)), y] = 1.0\n",
        "    return Y\n",
        "\n",
        "# Activation functions and derivatives\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "def sigmoid_grad(x):\n",
        "    s = sigmoid(x)\n",
        "    return s * (1 - s)\n",
        "\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "\n",
        "def tanh_grad(x):\n",
        "    return 1 - np.tanh(x) ** 2\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def relu_grad(x):\n",
        "    return (x > 0).astype(x.dtype)\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "    # x shape: (N, C)\n",
        "    z = x - np.max(x, axis=1, keepdims=True)\n",
        "    exp = np.exp(z)\n",
        "    return exp / np.sum(exp, axis=1, keepdims=True)\n",
        "\n",
        "# Losses and gradients\n",
        "\n",
        "def mse_loss(y_pred, y_true):\n",
        "    # both shapes (N, C) or (N,1)\n",
        "    N = y_pred.shape[0]\n",
        "    loss = 0.5 * np.sum((y_pred - y_true) ** 2) / N\n",
        "    return loss\n",
        "\n",
        "\n",
        "def mse_grad(y_pred, y_true):\n",
        "    N = y_pred.shape[0]\n",
        "    return (y_pred - y_true) / N\n",
        "\n",
        "\n",
        "def cross_entropy_loss_logits(logits, y_true_onehot):\n",
        "    # logits: raw outputs before softmax; y_true_onehot: one-hot\n",
        "    probs = softmax(logits)\n",
        "    N = logits.shape[0]\n",
        "    clipped = np.clip(probs, 1e-12, 1.0)\n",
        "    loss = -np.sum(y_true_onehot * np.log(clipped)) / N\n",
        "    return loss\n",
        "\n",
        "\n",
        "def grad_softmax_cross_entropy(logits, y_true_onehot):\n",
        "    # returns gradient dL/dlogits for batch\n",
        "    probs = softmax(logits)\n",
        "    return (probs - y_true_onehot) / logits.shape[0]\n",
        "\n",
        "# ------------------- Neural Network (manual backprop) -------------------\n",
        "class SimpleMLP:\n",
        "    def __init__(self, input_dim, hidden_sizes, output_dim, activation='tanh', weight_scale=None):\n",
        "        self.sizes = [input_dim] + hidden_sizes + [output_dim]\n",
        "        self.num_layers = len(self.sizes) - 1\n",
        "        self.activation_name = activation\n",
        "\n",
        "        # initialize weights and biases (Xavier)\n",
        "        self.W = []\n",
        "        self.b = []\n",
        "        for i in range(self.num_layers):\n",
        "            fan_in = self.sizes[i]\n",
        "            fan_out = self.sizes[i+1]\n",
        "            limit = np.sqrt(6.0 / (fan_in + fan_out))\n",
        "            W = np.random.uniform(-limit, limit, size=(fan_in, fan_out))\n",
        "            b = np.zeros((1, fan_out))\n",
        "            self.W.append(W)\n",
        "            self.b.append(b)\n",
        "\n",
        "        # velocity for momentum\n",
        "        self.vW = [np.zeros_like(W) for W in self.W]\n",
        "        self.vb = [np.zeros_like(b) for b in self.b]\n",
        "\n",
        "    def activation(self, x):\n",
        "        if self.activation_name == 'sigmoid':\n",
        "            return sigmoid(x)\n",
        "        elif self.activation_name == 'tanh':\n",
        "            return tanh(x)\n",
        "        elif self.activation_name == 'relu':\n",
        "            return relu(x)\n",
        "        else:\n",
        "            raise ValueError('Unknown activation')\n",
        "\n",
        "    def activation_grad(self, x):\n",
        "        if self.activation_name == 'sigmoid':\n",
        "            return sigmoid_grad(x)\n",
        "        elif self.activation_name == 'tanh':\n",
        "            return tanh_grad(x)\n",
        "        elif self.activation_name == 'relu':\n",
        "            return relu_grad(x)\n",
        "        else:\n",
        "            raise ValueError('Unknown activation')\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"Forward pass. Returns logits (before softmax for classification) and caches.\n",
        "        caches: list of tuples (z, a) where z = W^T a_prev + b, a = activation(z) (for hidden layers)\n",
        "        \"\"\"\n",
        "        a = X\n",
        "        caches = []\n",
        "        for i in range(self.num_layers - 1):  # hidden layers\n",
        "            z = a.dot(self.W[i]) + self.b[i]\n",
        "            a = self.activation(z)\n",
        "            caches.append((z, a))\n",
        "        # output layer (logits)\n",
        "        z = a.dot(self.W[-1]) + self.b[-1]\n",
        "        caches.append((z, None))  # None placeholder for activation on output\n",
        "        return z, caches\n",
        "\n",
        "    def predict(self, X):\n",
        "        logits, _ = self.forward(X)\n",
        "        if LOSS == 'mse':\n",
        "            return logits\n",
        "        else:\n",
        "            return softmax(logits)\n",
        "\n",
        "    def compute_loss_and_grads(self, X, y):\n",
        "        \"\"\"Compute loss and gradients for a batch (vectorized).\n",
        "        y: either one-hot (for cross-entropy) or continuous targets for MSE\n",
        "        Returns: loss, grads_W, grads_b\n",
        "        \"\"\"\n",
        "        logits, caches = self.forward(X)\n",
        "        grads_W = [np.zeros_like(W) for W in self.W]\n",
        "        grads_b = [np.zeros_like(b) for b in self.b]\n",
        "\n",
        "        if LOSS == 'mse':\n",
        "            # predictions are logits directly\n",
        "            preds = logits\n",
        "            loss = mse_loss(preds, y)\n",
        "            delta = mse_grad(preds, y)  # shape (N, C)\n",
        "        else:\n",
        "            # cross-entropy with softmax\n",
        "            loss = cross_entropy_loss_logits(logits, y)\n",
        "            delta = grad_softmax_cross_entropy(logits, y)  # dL/dlogits\n",
        "\n",
        "        # gradient for output layer\n",
        "        a_prev = caches[-2][1] if self.num_layers > 1 else X\n",
        "        grads_W[-1] = a_prev.T.dot(delta)\n",
        "        grads_b[-1] = np.sum(delta, axis=0, keepdims=True)\n",
        "\n",
        "        # backprop through hidden layers\n",
        "        delta_prev = delta\n",
        "        for l in range(self.num_layers - 2, -1, -1):\n",
        "            z_l, a_l = caches[l]\n",
        "            if l == 0:\n",
        "                a_prev = X\n",
        "            else:\n",
        "                a_prev = caches[l-1][1]\n",
        "            W_next = self.W[l+1]\n",
        "            # propagate delta\n",
        "            delta = delta_prev.dot(W_next.T) * self.activation_grad(z_l)\n",
        "            grads_W[l] = a_prev.T.dot(delta)\n",
        "            grads_b[l] = np.sum(delta, axis=0, keepdims=True)\n",
        "            delta_prev = delta\n",
        "\n",
        "        return loss, grads_W, grads_b\n",
        "\n",
        "    def update_params(self, grads_W, grads_b, lr, momentum):\n",
        "        for i in range(self.num_layers):\n",
        "            if momentum > 0:\n",
        "                self.vW[i] = momentum * self.vW[i] - lr * grads_W[i]\n",
        "                self.vb[i] = momentum * self.vb[i] - lr * grads_b[i]\n",
        "                self.W[i] += self.vW[i]\n",
        "                self.b[i] += self.vb[i]\n",
        "            else:\n",
        "                self.W[i] -= lr * grads_W[i]\n",
        "                self.b[i] -= lr * grads_b[i]\n",
        "\n",
        "# ------------------- Gradient checking (finite differences) -------------------\n",
        "\n",
        "def grad_check(model, X, y, epsilon=1e-5, tol=1e-6):\n",
        "    \"\"\"Performs gradient checking on model parameters for a small batch X,y.\n",
        "    Compares analytical gradients to numerical finite-diff gradients.\n",
        "    \"\"\"\n",
        "    _, analytic_grads_W, analytic_grads_b = model.compute_loss_and_grads(X, y)\n",
        "\n",
        "    # check W's\n",
        "    for idx, W in enumerate(model.W):\n",
        "        W_shape = W.shape\n",
        "        numeric_grad = np.zeros_like(W)\n",
        "        it = np.nditer(W, flags=['multi_index'], op_flags=['readwrite'])\n",
        "        while not it.finished:\n",
        "            ix = it.multi_index\n",
        "            orig = W[ix]\n",
        "            W[ix] = orig + epsilon\n",
        "            loss_plus, _, _ = model.compute_loss_and_grads(X, y)\n",
        "            W[ix] = orig - epsilon\n",
        "            loss_minus, _, _ = model.compute_loss_and_grads(X, y)\n",
        "            W[ix] = orig\n",
        "            numeric_grad[ix] = (loss_plus - loss_minus) / (2 * epsilon)\n",
        "            it.iternext()\n",
        "        diff = np.linalg.norm(analytic_grads_W[idx] - numeric_grad) / (np.linalg.norm(analytic_grads_W[idx]) + np.linalg.norm(numeric_grad) + 1e-12)\n",
        "        print(f'Grad check W[{idx}]: relative difference = {diff:.8e}')\n",
        "        if diff > tol:\n",
        "            print('WARNING: gradient check failed for W', idx)\n",
        "\n",
        "    # check b's\n",
        "    for idx, b in enumerate(model.b):\n",
        "        b_shape = b.shape\n",
        "        numeric_grad = np.zeros_like(b)\n",
        "        it = np.nditer(b, flags=['multi_index'], op_flags=['readwrite'])\n",
        "        while not it.finished:\n",
        "            ix = it.multi_index\n",
        "            orig = b[ix]\n",
        "            b[ix] = orig + epsilon\n",
        "            loss_plus, _, _ = model.compute_loss_and_grads(X, y)\n",
        "            b[ix] = orig - epsilon\n",
        "            loss_minus, _, _ = model.compute_loss_and_grads(X, y)\n",
        "            b[ix] = orig\n",
        "            numeric_grad[ix] = (loss_plus - loss_minus) / (2 * epsilon)\n",
        "            it.iternext()\n",
        "        diff = np.linalg.norm(analytic_grads_b[idx] - numeric_grad) / (np.linalg.norm(analytic_grads_b[idx]) + np.linalg.norm(numeric_grad) + 1e-12)\n",
        "        print(f'Grad check b[{idx}]: relative difference = {diff:.8e}')\n",
        "        if diff > tol:\n",
        "            print('WARNING: gradient check failed for b', idx)\n",
        "\n",
        "# ------------------- Small examples / tests -------------------\n",
        "\n",
        "def run_xor_example():\n",
        "    print('\\nRunning XOR example (binary classification)...')\n",
        "    # XOR dataset\n",
        "    X = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=np.float32)\n",
        "    y = np.array([0,1,1,0], dtype=np.int32)\n",
        "    y_onehot = one_hot(y, 2)\n",
        "\n",
        "    model = SimpleMLP(input_dim=2, hidden_sizes=HIDDEN_SIZES, output_dim=2, activation=ACTIVATION)\n",
        "\n",
        "    if GRAD_CHECK:\n",
        "        print('Running gradient check on XOR...')\n",
        "        grad_check(model, X, y_onehot)\n",
        "\n",
        "    # training loop\n",
        "    for epoch in range(1, EPOCHS+1):\n",
        "        # simple full-batch training for XOR\n",
        "        loss, grads_W, grads_b = model.compute_loss_and_grads(X, y_onehot)\n",
        "        model.update_params(grads_W, grads_b, LEARNING_RATE, MOMENTUM)\n",
        "        if epoch % PRINT_EVERY == 0 or epoch == 1:\n",
        "            preds = model.predict(X)\n",
        "            acc = np.mean(np.argmax(preds, axis=1) == y)\n",
        "            print(f'Epoch {epoch} - loss: {loss:.6f} - acc: {acc*100:.2f}%')\n",
        "    print('Final predictions:', np.argmax(model.predict(X), axis=1))\n",
        "\n",
        "\n",
        "def run_synthetic_multiclass():\n",
        "    print('\\nRunning synthetic 3-class classification example...')\n",
        "    # create 3 Gaussian blobs in 2D\n",
        "    N = 300\n",
        "    D = 2\n",
        "    K = 3\n",
        "    X = np.zeros((N*K, D))\n",
        "    y = np.zeros(N*K, dtype=np.int32)\n",
        "    for j in range(K):\n",
        "        ix = range(N*j, N*(j+1))\n",
        "        r = np.linspace(0.0,1,N)\n",
        "        t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.5\n",
        "        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
        "        y[ix] = j\n",
        "\n",
        "    # shuffle\n",
        "    perm = np.random.permutation(len(X))\n",
        "    X = X[perm]\n",
        "    y = y[perm]\n",
        "\n",
        "    # train/val split\n",
        "    split = int(0.8 * len(X))\n",
        "    X_train, X_val = X[:split], X[split:]\n",
        "    y_train, y_val = y[:split], y[split:]\n",
        "    y_train_oh = one_hot(y_train, K)\n",
        "    y_val_oh = one_hot(y_val, K)\n",
        "\n",
        "    model = SimpleMLP(input_dim=D, hidden_sizes=HIDDEN_SIZES, output_dim=K, activation=ACTIVATION)\n",
        "\n",
        "    if GRAD_CHECK:\n",
        "        print('Running gradient check on small subset...')\n",
        "        grad_check(model, X_train[:8], y_train_oh[:8])\n",
        "\n",
        "    # training with mini-batches\n",
        "    num_batches = int(np.ceil(X_train.shape[0] / BATCH_SIZE))\n",
        "    for epoch in range(1, EPOCHS+1):\n",
        "        perm = np.random.permutation(X_train.shape[0])\n",
        "        X_train_shuffled = X_train[perm]\n",
        "        y_train_shuffled = y_train_oh[perm]\n",
        "        epoch_loss = 0.0\n",
        "        for i in range(num_batches):\n",
        "            start = i * BATCH_SIZE\n",
        "            end = start + BATCH_SIZE\n",
        "            X_batch = X_train_shuffled[start:end]\n",
        "            y_batch = y_train_shuffled[start:end]\n",
        "            loss, grads_W, grads_b = model.compute_loss_and_grads(X_batch, y_batch)\n",
        "            model.update_params(grads_W, grads_b, LEARNING_RATE, MOMENTUM)\n",
        "            epoch_loss += loss\n",
        "        # eval\n",
        "        if epoch % PRINT_EVERY == 0 or epoch == 1:\n",
        "            preds_val = model.predict(X_val)\n",
        "            acc_val = np.mean(np.argmax(preds_val, axis=1) == y_val)\n",
        "            print(f'Epoch {epoch} - avg loss: {epoch_loss/num_batches:.6f} - val acc: {acc_val*100:.2f}%')\n",
        "\n",
        "    print('Final val acc:', np.mean(np.argmax(model.predict(X_val), axis=1) == y_val))\n",
        "\n",
        "# ------------------- Main -------------------\n",
        "if __name__ == '__main__':\n",
        "    if LOSS == 'mse' and HIDDEN_SIZES == []:\n",
        "        print('Warning: using MSE with no hidden layers is just linear regression')\n",
        "\n",
        "    # Run examples\n",
        "    run_xor_example()\n",
        "    run_synthetic_multiclass()\n",
        "\n",
        "    print('\\nDone.')\n"
      ],
      "metadata": {
        "id": "BS62Lbfe76Yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#27. Apply transfer learning using a pre-trained ResNet model for custom image classification.\n",
        "\"\"\"\n",
        "Transfer learning with a pre-trained ResNet (TensorFlow / Keras)\n",
        "\n",
        "Usage:\n",
        "    - Arrange your dataset directory like:\n",
        "        dataset/\n",
        "            train/\n",
        "                class_a/\n",
        "                class_b/\n",
        "                ...\n",
        "            val/\n",
        "                class_a/\n",
        "                class_b/\n",
        "                ...\n",
        "            test/\n",
        "                class_a/\n",
        "                class_b/\n",
        "                ...\n",
        "\n",
        "    - Edit HYPERPARAMS below (paths, epochs, batch size, fine-tune flag)\n",
        "    - Run: python transfer_learning_resnet_keras.py\n",
        "\n",
        "What it does:\n",
        " - Loads images with `image_dataset_from_directory` into tf.data pipelines\n",
        " - Builds a model using `tf.keras.applications.ResNet50` (imagenet weights) as a feature extractor\n",
        " - Trains the head first with frozen base, then optionally fine-tunes the top of the ResNet\n",
        " - Uses data augmentation layers, callbacks, and saves the best model\n",
        "\n",
        "Notes:\n",
        " - Requires TensorFlow 2.6+ (for keras preprocessing & layers API). Tested with TF 2.12.\n",
        " - For large datasets / faster training, configure a GPU runtime and increase batch size.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "# -------------------- HYPERPARAMS --------------------\n",
        "DATA_DIR = 'dataset'  # root directory with train/val/test subfolders\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "INITIAL_EPOCHS = 10\n",
        "FINE_TUNE_EPOCHS = 10\n",
        "LEARNING_RATE_HEAD = 1e-3\n",
        "LEARNING_RATE_FINE = 1e-4\n",
        "WEIGHTS = 'imagenet'  # or None to train from scratch (not recommended)\n",
        "RESNET_VERSION = 'ResNet50'  # options: ResNet50, ResNet50V2\n",
        "POOLING = 'avg'  # 'avg' or 'max'\n",
        "DROPOUT_RATE = 0.5\n",
        "FINE_TUNE_AT = 140  # layer index at which to start fine-tuning; None to skip fine-tune\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "MODEL_DIR = 'resnet_transfer_model'\n",
        "SEED = 42\n",
        "\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "# -------------------- Data pipelines --------------------\n",
        "print('Creating datasets from:', DATA_DIR)\n",
        "train_dir = os.path.join(DATA_DIR, 'train')\n",
        "val_dir = os.path.join(DATA_DIR, 'val')\n",
        "test_dir = os.path.join(DATA_DIR, 'test')\n",
        "\n",
        "if not os.path.exists(train_dir) or not os.path.exists(val_dir):\n",
        "    raise FileNotFoundError('Please create train/val (and optionally test) folders under dataset/ with class subfolders.')\n",
        "\n",
        "train_ds = keras.preprocessing.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=IMG_SIZE,\n",
        "    shuffle=True,\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "val_ds = keras.preprocessing.image_dataset_from_directory(\n",
        "    val_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    image_size=IMG_SIZE,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "if os.path.exists(test_dir):\n",
        "    test_ds = keras.preprocessing.image_dataset_from_directory(\n",
        "        test_dir,\n",
        "        labels='inferred',\n",
        "        label_mode='categorical',\n",
        "        batch_size=BATCH_SIZE,\n",
        "        image_size=IMG_SIZE,\n",
        "        shuffle=False\n",
        "    )\n",
        "else:\n",
        "    test_ds = None\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "num_classes = len(class_names)\n",
        "print('Classes:', class_names)\n",
        "\n",
        "# Prefetch for performance\n",
        "train_ds = train_ds.prefetch(AUTOTUNE)\n",
        "val_ds = val_ds.prefetch(AUTOTUNE)\n",
        "if test_ds is not None:\n",
        "    test_ds = test_ds.prefetch(AUTOTUNE)\n",
        "\n",
        "# -------------------- Data augmentation & preprocessing --------------------\n",
        "# Use simple augmentation; you can extend this (mixup, cutmix, RandAugment, etc.)\n",
        "data_augmentation = keras.Sequential([\n",
        "    layers.RandomFlip('horizontal'),\n",
        "    layers.RandomRotation(0.05),\n",
        "    layers.RandomTranslation(0.06, 0.06),\n",
        "], name='data_augmentation')\n",
        "\n",
        "# Use ResNet preprocessing (scale pixels as required)\n",
        "preprocess_input = None\n",
        "if RESNET_VERSION == 'ResNet50' or RESNET_VERSION == 'ResNet50V2':\n",
        "    from tensorflow.keras.applications.resnet import preprocess_input as resnet_preprocess\n",
        "    preprocess_input = resnet_preprocess\n",
        "else:\n",
        "    # fallback: simple rescale\n",
        "    preprocess_input = lambda x: x\n",
        "\n",
        "# Apply preprocessing in dataset pipeline\n",
        "def prepare(ds, training=False):\n",
        "    def _map_fn(x, y):\n",
        "        x = tf.cast(x, tf.float32)\n",
        "        x = preprocess_input(x)\n",
        "        if training:\n",
        "            x = data_augmentation(x)\n",
        "        return x, y\n",
        "    return ds.map(_map_fn, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "train_ds_proc = prepare(train_ds, training=True)\n",
        "val_ds_proc = prepare(val_ds, training=False)\n",
        "if test_ds is not None:\n",
        "    test_ds_proc = prepare(test_ds, training=False)\n",
        "else:\n",
        "    test_ds_proc = None\n",
        "\n",
        "# -------------------- Build model --------------------\n",
        "print('Building model...')\n",
        "if RESNET_VERSION == 'ResNet50V2':\n",
        "    base_model = keras.applications.ResNet50V2(weights=WEIGHTS, include_top=False, input_shape=(*IMG_SIZE, 3))\n",
        "else:\n",
        "    base_model = keras.applications.ResNet50(weights=WEIGHTS, include_top=False, input_shape=(*IMG_SIZE, 3))\n",
        "\n",
        "# Freeze the base\n",
        "base_model.trainable = False\n",
        "\n",
        "inputs = keras.Input(shape=(*IMG_SIZE, 3))\n",
        "# optional: include augmentation in model so it also runs during .predict() for debugging\n",
        "x = inputs\n",
        "x = data_augmentation(x)\n",
        "x = preprocess_input(x)\n",
        "\n",
        "x = base_model(x, training=False)\n",
        "if POOLING == 'avg':\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "else:\n",
        "    x = layers.GlobalMaxPooling2D()(x)\n",
        "\n",
        "x = layers.Dropout(DROPOUT_RATE)(x)\n",
        "x = layers.Dense(256, activation='relu')(x)\n",
        "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.summary()\n",
        "\n",
        "# -------------------- Compile & callbacks --------------------\n",
        "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE_HEAD)\n",
        "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "now = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "checkpoint_path = os.path.join(MODEL_DIR, f'best_resnet_{now}.h5')\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(checkpoint_path, monitor='val_accuracy', save_best_only=True, verbose=1),\n",
        "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, verbose=1),\n",
        "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1)\n",
        "]\n",
        "\n",
        "# Compute steps per epoch (optional)\n",
        "train_steps = tf.data.experimental.cardinality(train_ds).numpy()\n",
        "val_steps = tf.data.experimental.cardinality(val_ds).numpy()\n",
        "print(f'Train batches: {train_steps}, Val batches: {val_steps}')\n",
        "\n",
        "# -------------------- Train head --------------------\n",
        "print('\\nTraining head (base frozen) for', INITIAL_EPOCHS, 'epochs...')\n",
        "history_head = model.fit(\n",
        "    train_ds_proc,\n",
        "    epochs=INITIAL_EPOCHS,\n",
        "    validation_data=val_ds_proc,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# -------------------- Optional fine-tuning --------------------\n",
        "if FINE_TUNE_AT is not None:\n",
        "    print('\\nStarting fine-tuning...')\n",
        "    # Unfreeze from layer index FINE_TUNE_AT onwards\n",
        "    base_model.trainable = True\n",
        "    # Freeze earlier layers\n",
        "    for i, layer in enumerate(base_model.layers):\n",
        "        if i < FINE_TUNE_AT:\n",
        "            layer.trainable = False\n",
        "        else:\n",
        "            layer.trainable = True\n",
        "\n",
        "    # recompile with a lower LR\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE_FINE),\n",
        "                  loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    history_fine = model.fit(\n",
        "        train_ds_proc,\n",
        "        epochs=INITIAL_EPOCHS + FINE_TUNE_EPOCHS,\n",
        "        initial_epoch=history_head.epoch[-1] if len(history_head.epoch) > 0 else 0,\n",
        "        validation_data=val_ds_proc,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "# -------------------- Evaluate on test set --------------------\n",
        "if test_ds_proc is not None:\n",
        "    print('\\nEvaluating on test set...')\n",
        "    best_model = keras.models.load_model(checkpoint_path)\n",
        "    test_loss, test_acc = best_model.evaluate(test_ds_proc)\n",
        "    print(f'Test loss: {test_loss:.4f} - Test acc: {test_acc*100:.2f}%')\n",
        "\n",
        "# -------------------- Save final model and class mapping --------------------\n",
        "final_path = os.path.join(MODEL_DIR, f'final_resnet_{now}.h5')\n",
        "model.save(final_path)\n",
        "print('Saved final model to', final_path)\n",
        "\n",
        "# Save class names\n",
        "import json\n",
        "with open(os.path.join(MODEL_DIR, 'class_names.json'), 'w') as f:\n",
        "    json.dump(class_names, f)\n",
        "print('Saved class mapping (class_names.json)')\n",
        "\n",
        "print('\\nDone.')\n"
      ],
      "metadata": {
        "id": "EGHM1ubk76VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#28. Train a Transformer-based model (HuggingFace) for text classification.\n",
        "\"\"\"\n",
        "Fine-tune a Transformer (Hugging Face) for text classification\n",
        "\n",
        "Usage examples:\n",
        "  # Fine-tune on GLUE (sst2) via datasets\n",
        "  python transformer_text_classification_hf.py --dataset_name glue --dataset_config_name sst2\n",
        "\n",
        "  # Fine-tune on local CSV (columns: text,label)\n",
        "  python transformer_text_classification_hf.py --train_file ./train.csv --validation_file ./val.csv --text_column text --label_column label\n",
        "\n",
        "Requirements:\n",
        "  pip install transformers datasets evaluate accelerate\n",
        "\n",
        "What it does:\n",
        " - Loads dataset (Hugging Face datasets or local CSV/JSON)\n",
        " - Auto-detects number of labels and builds label mapping\n",
        " - Tokenizes with a selected pretrained model tokenizer\n",
        " - Builds a model for sequence classification from a pretrained checkpoint\n",
        " - Fine-tunes using Trainer API with evaluation metrics (accuracy, f1)\n",
        " - Saves the best checkpoint and final model\n",
        "\n",
        "Notes:\n",
        " - This script uses the Trainer API for convenience. For large datasets / advanced workflows\n",
        "   consider using Accelerate or custom training loops.\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional, Dict\n",
        "\n",
        "import numpy as np\n",
        "from datasets import load_dataset, DatasetDict\n",
        "import evaluate\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        "    set_seed,\n",
        ")\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description='Fine-tune a transformer for text classification')\n",
        "\n",
        "    # dataset options\n",
        "    parser.add_argument('--dataset_name', type=str, default=None,\n",
        "                        help='HuggingFace dataset name (e.g. glue). If provided, dataset_config_name may be needed.')\n",
        "    parser.add_argument('--dataset_config_name', type=str, default=None,\n",
        "                        help='Config name for dataset (e.g. sst2 for glue)')\n",
        "    parser.add_argument('--train_file', type=str, default=None, help='Local train file (csv or json)')\n",
        "    parser.add_argument('--validation_file', type=str, default=None, help='Local validation file (csv or json)')\n",
        "    parser.add_argument('--text_column', type=str, default='text', help='Name of the text column in local files')\n",
        "    parser.add_argument('--label_column', type=str, default='label', help='Name of the label column in local files')\n",
        "\n",
        "    # model / training options\n",
        "    parser.add_argument('--model_name_or_path', type=str, default='distilbert-base-uncased',\n",
        "                        help='Pretrained model identifier from huggingface.co/models')\n",
        "    parser.add_argument('--output_dir', type=str, default='./hf_text_classifier', help='Where to store checkpoints')\n",
        "    parser.add_argument('--max_length', type=int, default=128, help='Max sequence length for tokenization')\n",
        "    parser.add_argument('--per_device_train_batch_size', type=int, default=16)\n",
        "    parser.add_argument('--per_device_eval_batch_size', type=int, default=32)\n",
        "    parser.add_argument('--learning_rate', type=float, default=5e-5)\n",
        "    parser.add_argument('--weight_decay', type=float, default=0.0)\n",
        "    parser.add_argument('--num_train_epochs', type=int, default=3)\n",
        "    parser.add_argument('--seed', type=int, default=42)\n",
        "    parser.add_argument('--metric_for_best_model', type=str, default='eval_accuracy')\n",
        "    parser.add_argument('--greater_is_better', action='store_true', help='Whether larger metric is better')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "    set_seed(args.seed)\n",
        "\n",
        "    # ----- Load dataset -----\n",
        "    if args.dataset_name is not None:\n",
        "        print(f'Loading dataset {args.dataset_name} {args.dataset_config_name or \"\"}...')\n",
        "        raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)\n",
        "        # Expect split names train/validation/test depending on dataset\n",
        "    elif args.train_file is not None and args.validation_file is not None:\n",
        "        data_files = { 'train': args.train_file, 'validation': args.validation_file }\n",
        "        print('Loading local files:', data_files)\n",
        "        raw_datasets = load_dataset('csv' if args.train_file.endswith('.csv') else 'json', data_files=data_files)\n",
        "    else:\n",
        "        raise ValueError('You must provide either --dataset_name or both --train_file and --validation_file')\n",
        "\n",
        "    # Normalize dataset splits: ensure 'train' and 'validation' split exist\n",
        "    if 'train' not in raw_datasets:\n",
        "        raise ValueError('Dataset must contain a \"train\" split')\n",
        "    if 'validation' not in raw_datasets:\n",
        "        # if only train/test, split train into train/val\n",
        "        if 'test' in raw_datasets:\n",
        "            raw_datasets = DatasetDict({ 'train': raw_datasets['train'], 'validation': raw_datasets['test'] })\n",
        "        else:\n",
        "            # create small validation from train\n",
        "            raw_datasets = raw_datasets['train'].train_test_split(test_size=0.1)\n",
        "\n",
        "    # ----- Inspect labels and text column -----\n",
        "    # Try to detect label column\n",
        "    first_train_example = raw_datasets['train'][0]\n",
        "    print('Example train row keys:', list(first_train_example.keys()))\n",
        "\n",
        "    text_column = args.text_column\n",
        "    label_column = args.label_column\n",
        "    if text_column not in first_train_example:\n",
        "        # try common alternatives\n",
        "        alt_text = [k for k in first_train_example.keys() if k.lower() in ('sentence','text','review','utterance')]\n",
        "        if alt_text:\n",
        "            text_column = alt_text[0]\n",
        "            print(f'Auto-detected text column: {text_column}')\n",
        "        else:\n",
        "            raise ValueError('Could not find a text column. Provide --text_column')\n",
        "\n",
        "    if label_column not in first_train_example:\n",
        "        # try common alternatives\n",
        "        alt_label = [k for k in first_train_example.keys() if k.lower() in ('label','labels','rating','stars')]\n",
        "        if alt_label:\n",
        "            label_column = alt_label[0]\n",
        "            print(f'Auto-detected label column: {label_column}')\n",
        "        else:\n",
        "            raise ValueError('Could not find a label column. Provide --label_column')\n",
        "\n",
        "    # If labels are strings -> create mapping\n",
        "    is_label_str = isinstance(first_train_example[label_column], str)\n",
        "    if is_label_str:\n",
        "        labels = sorted(list({ex[label_column] for ex in raw_datasets['train']}))\n",
        "        label2id = {label: i for i, label in enumerate(labels)}\n",
        "        id2label = {i: label for label, i in label2id.items()}\n",
        "        def map_label(example):\n",
        "            example['label'] = label2id[example[label_column]]\n",
        "            return example\n",
        "        raw_datasets = raw_datasets.map(map_label)\n",
        "        num_labels = len(labels)\n",
        "        print('Detected string labels. Mapping provided. Num labels =', num_labels)\n",
        "    else:\n",
        "        # assume labels are already ints in [0..K-1]\n",
        "        unique_labels = sorted(list(set(raw_datasets['train'][label_column])))\n",
        "        num_labels = len(unique_labels)\n",
        "        print('Detected numeric labels. Num labels =', num_labels)\n",
        "        # ensure column named 'label'\n",
        "        if label_column != 'label':\n",
        "            raw_datasets = raw_datasets.rename_column(label_column, 'label')\n",
        "\n",
        "    # ----- Load tokenizer & model -----\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        num_labels=num_labels,\n",
        "    )\n",
        "\n",
        "    # ----- Tokenize -----\n",
        "    def preprocess_fn(examples):\n",
        "        return tokenizer(examples[text_column], truncation=True, max_length=args.max_length)\n",
        "\n",
        "    tokenized = raw_datasets.map(preprocess_fn, batched=True)\n",
        "\n",
        "    # ----- Data collator -----\n",
        "    data_collator = DataCollatorWithPadding(tokenizer)\n",
        "\n",
        "    # ----- Evaluation metric -----\n",
        "    accuracy = evaluate.load('accuracy')\n",
        "    f1 = evaluate.load('f1')\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        predictions = np.argmax(logits, axis=-1)\n",
        "        acc = accuracy.compute(predictions=predictions, references=labels)\n",
        "        f1_macro = f1.compute(predictions=predictions, references=labels, average='macro')\n",
        "        return {\n",
        "            'accuracy': acc['accuracy'],\n",
        "            'f1_macro': f1_macro['f1']\n",
        "        }\n",
        "\n",
        "    # ----- TrainingArguments & Trainer -----\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=args.output_dir,\n",
        "        evaluation_strategy='epoch',\n",
        "        save_strategy='epoch',\n",
        "        learning_rate=args.learning_rate,\n",
        "        per_device_train_batch_size=args.per_device_train_batch_size,\n",
        "        per_device_eval_batch_size=args.per_device_eval_batch_size,\n",
        "        num_train_epochs=args.num_train_epochs,\n",
        "        weight_decay=args.weight_decay,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=args.metric_for_best_model.replace('eval_',''),\n",
        "        greater_is_better=args.greater_is_better,\n",
        "        save_total_limit=2,\n",
        "        seed=args.seed,\n",
        "        fp16=False,\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized['train'],\n",
        "        eval_dataset=tokenized['validation'],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # ----- Train -----\n",
        "    trainer.train()\n",
        "\n",
        "    # ----- Evaluate on validation (and test if present) -----\n",
        "    print('\\nValidation results:')\n",
        "    print(trainer.evaluate(tokenized['validation']))\n",
        "\n",
        "    if 'test' in tokenized:\n",
        "        print('\\nTest results:')\n",
        "        print(trainer.evaluate(tokenized['test']))\n",
        "\n",
        "    # ----- Save model & tokenizer -----\n",
        "    trainer.save_model(args.output_dir)\n",
        "    tokenizer.save_pretrained(args.output_dir)\n",
        "    print(f'Saved model and tokenizer to {args.output_dir}')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "aE1U4r3976LB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}